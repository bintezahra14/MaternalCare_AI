{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bintezahra14/MaternalCare_AI/blob/main/MD_Data_Miner_ITAI2377_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0qznJ7C9MUxI",
      "metadata": {
        "id": "0qznJ7C9MUxI"
      },
      "source": [
        "# MaternalCare AI Assistant\n",
        "\n",
        "An AI-Driven System to Address Maternal Health Disparities in the U.S.\n",
        "\n",
        "ðŸ“˜ Course: ITAI 2377 â€“ AI Capstone\n",
        "\n",
        "Group Name: Data Miner\n",
        "\n",
        "Team Members: Zahra & Virginia\n",
        "\n",
        "Instructor: Maryam Esmali"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oc2iBS5aSf5g",
      "metadata": {
        "id": "oc2iBS5aSf5g"
      },
      "source": [
        "## 1. Executive Summary\n",
        "\n",
        "MaternalCare_AI is a domain-specific artificial intelligence (AI) assistant developed to bridge critical information gaps in maternal healthcare, particularly for individuals in underserved and high-risk communities. The assistant delivers accurate, accessible, and context-aware responses to maternal health queries by integrating structured geographic vulnerability data with a curated knowledge base of medical guidance.\n",
        "\n",
        "At the core of the system is a retrieval-augmented generation (RAG) architecture, which semantically matches user queries to embedded knowledge chunks derived from expert-verified documents. This retrieval is further enhanced by incorporating ZIP Code Tabulation Area (ZCTA) data, enabling the assistant to tailor responses based on localized health risk indicators such as the Social Vulnerability Index (SVI).\n",
        "\n",
        "The primary objective of MaternalCare_AI is to empower users with plain-language, trustworthy insights on topics such as prenatal care, postpartum symptoms, and available community resources. Its lightweight, modular design makes it deployable in low-resource environments, positioning it as a practical tool for improving maternal health literacy and support equity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DLpPW5jTM5pF",
      "metadata": {
        "id": "DLpPW5jTM5pF"
      },
      "source": [
        "# 2. Project Definition\n",
        "## 2.1 Domain Selection\n",
        "\n",
        "The maternal healthcare domain was chosen due to the persistent disparities in maternal health outcomes across geographically and socioeconomically vulnerable populations. Women in high-risk ZIP codes often face limited access to timely and reliable health information. This project addresses the urgent need for culturally appropriate, accessible, and geographically personalized maternal health support through an AI-powered assistant.\n",
        "\n",
        "## 2.2 Problem Statement\n",
        "\n",
        "Despite the availability of online health resources, many pregnant individuals especially those in marginalized communitiesâ€”lack access to trustworthy, understandable, and relevant maternal health guidance. Existing materials are frequently fragmented, overly clinical, or disconnected from the lived experiences of their intended users. There is a critical gap in tools that combine localized risk awareness with plain-language health communication.\n",
        "\n",
        "## 2.3 Core Functionalities\n",
        "\n",
        "**Localized Health Information**\n",
        "Uses ZIP Code Tabulation Area (ZCTA) and CDCâ€™s Social Vulnerability Index (SVI) to tailor responses based on geographic health risk levels.\n",
        "\n",
        "**Knowledge Base Q&A**\n",
        "Implements a retrieval-augmented generation (RAG) system to match user queries with semantically embedded knowledge chunks for context-relevant answers.\n",
        "\n",
        "**Multi-Source Knowledge Integration**\n",
        "Combines public health documents, articles, and FAQs into a unified, embedded knowledge base to enhance response quality and coverage.\n",
        "\n",
        "**Performance Evaluation**\n",
        "Applies a structured golden question-answer set to evaluate semantic accuracy and retrieval effectiveness.\n",
        "\n",
        "## 2.4 Target Users\n",
        "\n",
        "* Pregnant individuals and postpartum mothers seeking accessible health advice\n",
        "\n",
        "* Community health workers supporting maternal care in under-resourced areas\n",
        "\n",
        "* Nonprofits, advocacy groups, and public health organizations involved in maternal wellness\n",
        "\n",
        "* Government agencies focused on maternal health education and intervention\n",
        "\n",
        "## 2.5 Value Proposition\n",
        "\n",
        "MaternalCare_AI provides a scalable, interpretable, and equity-focused solution to maternal health information access. By combining location-based vulnerability data with retrieval-augmented AI, it delivers relevant, plain-language guidance to users who may otherwise face barriers to care. The system is fully deployable in low-resource settings (e.g., via Google Colab), making it accessible for use in community outreach, mobile health clinics, and digital health platforms."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6816986",
      "metadata": {
        "id": "e6816986"
      },
      "source": [
        "## Setup (folders + packages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7539dcea",
      "metadata": {
        "id": "7539dcea"
      },
      "outputs": [],
      "source": [
        "# Create a local project structure under /content (no Drive needed)\n",
        "import os, sys, subprocess\n",
        "PROJECT = '/content/ITAI2377_MaternalCareAI_OneNotebook'\n",
        "for p in [PROJECT, f'{PROJECT}/data/raw', f'{PROJECT}/data/processed', f'{PROJECT}/data/embeddings', f'{PROJECT}/models', f'{PROJECT}/outputs']:\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "os.chdir(PROJECT)\n",
        "\n",
        "def pip_install(pkgs):\n",
        "    cmd = ['pip','install','-q'] + pkgs\n",
        "    print(\"Installing:\", \" \".join(pkgs))\n",
        "    subprocess.check_call(cmd)\n",
        "\n",
        "pip_install(['pandas','numpy','requests','sentence-transformers','chromadb','beautifulsoup4','pdfminer.six','tldextract','validators','tiktoken'])\n",
        "\n",
        "import pandas as pd, numpy as np\n",
        "print('âœ… Ready. Working dir:', os.getcwd())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V4lC0aG9TOzE",
      "metadata": {
        "id": "V4lC0aG9TOzE"
      },
      "source": [
        "# 3. Data Requirements Analysis\n",
        "## 3.1 Data Types\n",
        "\n",
        "**Structured:**\n",
        "Social Vulnerability Index (SVI) scores in CSV format, representing geographic vulnerability at the ZIP Code Tabulation Area (ZCTA) level. These values are used to personalize responses based on regional risk factors.\n",
        "\n",
        "**Unstructured:**\n",
        "Free-text maternal care guidance compiled from public domain resources. This content (e.g., kb_docs.csv) forms the retrieval-based knowledge base for the assistant and is preprocessed into semantic chunks for embedding and querying.\n",
        "\n",
        "## 3.2 Data Sources\n",
        "\n",
        "**SVI Dataset:**\n",
        "Sourced from the CDC/ATSDR Social Vulnerability Index via its open data portal, providing ZCTA-level indicators such as poverty, access to transportation, and housing conditions.\n",
        "\n",
        "**Knowledge Base:**\n",
        "Derived from a curated collection of public domain maternal health articles, FAQs, and guidelines. Sources include organizations like the WHO, Mayo Clinic, and U.S. Department of Health and Human Services.\n",
        "\n",
        "## 3.3 Data Volume & Velocity\n",
        "\n",
        "Static dataset (~30,000 ZCTAs; <100 documents)\n",
        "\n",
        "Updated manually or annually\n",
        "Approximately 30,000 ZCTA entries in the structured SVI dataset and fewer than 100 unstructured documents in the knowledge base. Datasets are largely static and updated manually or annually. No streaming or real-time ingestion is required.\n",
        "\n",
        "## 3.4 Data Quality Requirements\n",
        "\n",
        "Valid ZCTA codes (zero-padded)\n",
        "Clean, readable documents with a minimum word count\n",
        "\n",
        "No duplicate or irrelevant text chunks\n",
        "\n",
        "## 3.5 Data Challenges\n",
        "\n",
        "Inconsistent ZCTA formats\n",
        "\n",
        "Sparse SVI data for some ZIP codes\n",
        "\n",
        "Variable quality or length of unstructured KB documents\n",
        "\n",
        "## 3.6 Data Schema\n",
        "\n",
        "[SVI Table]\n",
        "ZCTA â†’ RPL_THEMES\n",
        "\n",
        "[KB Table]\n",
        "title â†’ text â†’ chunk â†’ embedding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b430543",
      "metadata": {
        "id": "9b430543"
      },
      "source": [
        "## Upload Datasets (Manual)\n",
        "Upload one or more CSVs, such as:\n",
        "- `SVI_2022_US_ZCTA.csv` (CDC/ATSDR SVI by ZCTA)\n",
        "- `kb_docs.csv` (two columns: `title`, `text`) for the knowledge base\n",
        "- Any other CSVs you want to explore\n",
        "\n",
        "Files will be placed into `data/raw/` automatically.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93c73d00",
      "metadata": {
        "id": "93c73d00"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os, pandas as pd\n",
        "\n",
        "uploaded = files.upload()  # Choose CSVs from your computer\n",
        "uploaded = files.upload()  # Choose CSVs from your computer\n",
        "uploaded = files.upload()  # Choose CSVs from your computer\n",
        "uploaded = files.upload()  # Choose CSVs from your computer\n",
        "for filename in uploaded.keys():\n",
        "    src = filename\n",
        "    dst = f'data/raw/{filename}'\n",
        "    os.replace(src, dst)\n",
        "    print('âœ… Saved to', dst)\n",
        "\n",
        "print('\\nðŸ“ Raw files in data/raw/:', os.listdir('data/raw'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38OIA9xNTwvi",
      "metadata": {
        "id": "38OIA9xNTwvi"
      },
      "source": [
        "# 4. Processing Pipeline Design\n",
        "## 4.1 Preprocessing Workflow\n",
        "\n",
        "Upload and validate SVI_2022_US_ZCTA.csv\n",
        "\n",
        "Clean columns, zero-pad ZCTA, filter key SVI metrics\n",
        "\n",
        "Upload and chunk kb_docs.csv using word-based overlap\n",
        "\n",
        "Generate sentence-level embeddings (MiniLM)\n",
        "\n",
        "Store embeddings in ChromaDB (vector store)\n",
        "\n",
        "## 4.2 Feature Engineering\n",
        "\n",
        "Text chunks (120-word segments with 20-word overlap)\n",
        "\n",
        "Embedding vectors via all-MiniLM-L6-v2\n",
        "\n",
        "ZCTA-matched filtering using SVI\n",
        "\n",
        "## 4.3 Pipeline Diagram (to include as figure)\n",
        "Raw CSVs\n",
        "   â†“\n",
        "[Cleaned SVI] + [Chunked Text]\n",
        "   â†“\n",
        "Sentence Embedding (MiniLM)\n",
        "   â†“\n",
        "Vector DB (ChromaDB)\n",
        "   â†“\n",
        "Query â†’ Retrieval â†’ Compose Answer\n",
        "\n",
        "## 4.4 Data Transformation Techniques\n",
        "\n",
        "Lowercasing, punctuation removal\n",
        "\n",
        "Word-based chunking with overlap\n",
        "\n",
        "Sentence embedding using pretrained model\n",
        "\n",
        "## 4.5 Infrastructure\n",
        "\n",
        "Runtime: Google Colab\n",
        "\n",
        "Storage: Local in-session (non-persistent)\n",
        "\n",
        "Libraries: pandas, sentence-transformers, chromadb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1a3fcf4",
      "metadata": {
        "id": "c1a3fcf4"
      },
      "source": [
        "## Inspect & Clean SVI (ZCTA-Level)\n",
        "- Detect ZCTA column name and zero-pad to 5 characters\n",
        "- Keep common SVI theme columns (you can modify this list)\n",
        "- Save clean file to `data/processed/svi_2022_clean.csv`\n",
        "\n",
        "> If your SVI file has a different name, set `SVI_FILE` below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caeb39ea",
      "metadata": {
        "id": "caeb39ea"
      },
      "outputs": [],
      "source": [
        "import pandas as pd, os\n",
        "\n",
        "# Change this if your uploaded file name is different\n",
        "SVI_FILE = 'SVI_2022_US_ZCTA.csv'\n",
        "# Correct the raw_path to load from /content directly\n",
        "raw_path = f'/content/{SVI_FILE}'\n",
        "assert os.path.exists(raw_path), f'Upload your SVI file as {raw_path} or update SVI_FILE.'\n",
        "\n",
        "df_svi = pd.read_csv(raw_path, low_memory=False)\n",
        "print('SVI raw shape:', df_svi.shape)\n",
        "display(df_svi.head(3))\n",
        "\n",
        "# Standardize columns\n",
        "df_svi.columns = [c.strip().lower() for c in df_svi.columns]\n",
        "\n",
        "# Try to detect ZCTA column\n",
        "zcta_col = None\n",
        "for cand in ['zcta','zcta5','zcta_5','fips','zcta5ce10']:\n",
        "    if cand in df_svi.columns:\n",
        "        zcta_col = cand\n",
        "        break\n",
        "if zcta_col is None:\n",
        "    raise ValueError('Could not detect the ZCTA column. Please check headers and set zcta_col manually.')\n",
        "\n",
        "# Zero-pad ZCTA\n",
        "df_svi[zcta_col] = df_svi[zcta_col].astype(str).str.zfill(5)\n",
        "\n",
        "# Choose SVI theme columns to keep (customize as needed)\n",
        "keep_like = ['rpl_themes','rpl_soc','rpl_house','rpl_minor','rpl_tran']\n",
        "keep = [zcta_col] + [c for c in df_svi.columns if any(k in c for k in keep_like)]\n",
        "df_svi_clean = df_svi[keep].drop_duplicates().dropna(how='all')\n",
        "\n",
        "os.makedirs('data/processed', exist_ok=True)\n",
        "out_svi = 'data/processed/svi_2022_clean.csv'\n",
        "df_svi_clean.to_csv(out_svi, index=False)\n",
        "print('âœ… Saved:', out_svi, df_svi_clean.shape)\n",
        "display(df_svi_clean.head(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aef79e41",
      "metadata": {
        "id": "aef79e41"
      },
      "source": [
        "## 5) Build Text Chunks for RAG\n",
        "We use a CSV named `kb_docs.csv` with two columns: `title`, `text`. If you didnâ€™t upload one, a tiny demo will be created.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zYfZLJt1UIU6",
      "metadata": {
        "id": "zYfZLJt1UIU6"
      },
      "source": [
        "# 5. Implementation Strategy\n",
        "## 5.1 Technical Stack\n",
        "\n",
        "Language: Python\n",
        "\n",
        "Libraries: pandas, numpy, sentence-transformers, chromadb, gradio\n",
        "\n",
        "Notebook Platform: Google Colab\n",
        "\n",
        "## 5.2 Stages\n",
        "\n",
        "1\tEnvironment setup, data upload\n",
        "2\tSVI cleaning, chunking\n",
        "3\tEmbedding + retrieval implementation\n",
        "4\tEvaluation, dashboard, final polish\n",
        "\n",
        "## 5.3 Team Responsibilities\n",
        "Member\tResponsibility\n",
        "Zahra\tData loading, preprocessing, Gradio UI\n",
        "Virginia\tChunking, embedding, retrieval logic\n",
        "\n",
        "## 5.4 Resource Needs\n",
        "\n",
        "Public datasets\n",
        "\n",
        "Free-tier Google Colab runtime\n",
        "\n",
        "No external API keys or LLM access needed\n",
        "\n",
        "## 5.5 Anticipated Challenges\n",
        "\n",
        "Chunking small documents may produce low-density embeddings\n",
        "\n",
        "Colab session resets (mitigated via checkpoint saving)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the updated kb_docs.csv file\n",
        "kb_path = '/content/ITAI2377_MaternalCareAI_OneNotebook/kb_docs.csv'  # Update path if needed\n",
        "kb = pd.read_csv(kb_path)\n",
        "kb.columns = [col.lower() for col in kb.columns]\n",
        "\n",
        "# Chunk the text\n",
        "def chunk_text(t, chunk_words=120, overlap=20):\n",
        "    words = str(t).split()\n",
        "    chunks = []\n",
        "    step = max(1, chunk_words - overlap)\n",
        "    for i in range(0, len(words), step):\n",
        "        chunk = \" \".join(words[i:i+chunk_words])\n",
        "        if len(chunk.split()) >= 20 or i == 0:\n",
        "            chunks.append(chunk)\n",
        "    return chunks\n",
        "\n",
        "# Build chunk records\n",
        "records = []\n",
        "for i, row in kb.iterrows():\n",
        "    chunks = chunk_text(row['text'])\n",
        "    for j, ch in enumerate(chunks):\n",
        "        records.append({\n",
        "            'doc_id': f'doc_{i}',\n",
        "            'chunk_id': f'doc_{i}_chunk_{j}',\n",
        "            'title': row['title'],\n",
        "            'chunk_text': ch\n",
        "        })\n",
        "\n",
        "chunks_df = pd.DataFrame(records)\n",
        "chunks_df.to_csv('/content/chunks.csv', index=False)\n",
        "print(\"âœ… Chunks Created:\", chunks_df.shape)"
      ],
      "metadata": {
        "id": "akzKASIHd3Mq"
      },
      "id": "akzKASIHd3Mq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8226eb99",
      "metadata": {
        "id": "8226eb99"
      },
      "source": [
        "## Create Embeddings (SentenceTransformers all-MiniLM-L6-v2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51330b71",
      "metadata": {
        "id": "51330b71"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "chunks_df = pd.read_csv('/content/chunks.csv')\n",
        "vecs = model.encode(chunks_df['chunk_text'].tolist(), show_progress_bar=True)\n",
        "\n",
        "np.save('/content/chunk_embeddings.npy', vecs)\n",
        "print(\"âœ… Embeddings Created:\", vecs.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c50454c1",
      "metadata": {
        "id": "c50454c1"
      },
      "source": [
        "## Load ChromaDB (Vector Database)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5755256f",
      "metadata": {
        "id": "5755256f"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "\n",
        "client = chromadb.PersistentClient(path=\"/content/chroma_db\")\n",
        "\n",
        "# Optional: delete previous collection\n",
        "try:\n",
        "    client.delete_collection('maternalcare_kb')\n",
        "except:\n",
        "    pass\n",
        "\n",
        "collection = client.create_collection(name='maternalcare_kb')\n",
        "\n",
        "# Reload chunks and embeddings\n",
        "chunks_df = pd.read_csv('/content/chunks.csv')\n",
        "vecs = np.load('/content/chunk_embeddings.npy')\n",
        "\n",
        "collection.add(\n",
        "    ids=chunks_df['chunk_id'].tolist(),\n",
        "    embeddings=vecs.tolist(),\n",
        "    documents=chunks_df['chunk_text'].tolist(),\n",
        "    metadatas=chunks_df[['doc_id','title']].to_dict('records')\n",
        ")\n",
        "print(\"âœ… ChromaDB Loaded with Vector Store\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3N5tXQL-UrZO",
      "metadata": {
        "id": "3N5tXQL-UrZO"
      },
      "source": [
        "# 6. Evaluation Framework\n",
        "## 6.1 Success Metrics\n",
        "\n",
        "Hit@3: Correct answer in top 3 retrieved chunks\n",
        "\n",
        "Manual QA Review: Reviewer confirms relevance\n",
        "\n",
        "UI Response Quality: Format, clarity, and usefulness\n",
        "\n",
        "## 6.2 Testing Strategy\n",
        "\n",
        "Compare system response with golden answer key\n",
        "\n",
        "Use embedded must-have keywords to score hits\n",
        "\n",
        "## 6.3 Feedback Mechanism\n",
        "\n",
        "Future integration with Gradio thumbs or survey\n",
        "\n",
        "Review flagged queries for content improvement\n",
        "\n",
        "## 6.4 Performance Benchmarks\n",
        "Metric\tTarget Value\n",
        "\n",
        "Hit@3 Accuracy\tâ‰¥ 80%\n",
        "\n",
        "Response Time\t< 2 sec/query\n",
        "\n",
        "Completion\t100% of test questions return an answer\n",
        "\n",
        "## 6.5 Continuous Improvement\n",
        "\n",
        "Add new documents as they are sourced\n",
        "\n",
        "Monitor feedback logs\n",
        "\n",
        "Re-embed if source texts are revised"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce686577",
      "metadata": {
        "id": "ce686577"
      },
      "source": [
        "## Retrieve & Compose an Answer (Prototype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ff7aad2",
      "metadata": {
        "id": "6ff7aad2"
      },
      "outputs": [],
      "source": [
        "# Compose response\n",
        "def compose_answer(query, results):\n",
        "    retrieved_docs = results.get('documents', [[]])[0]\n",
        "    if not retrieved_docs:\n",
        "        return f\"âš ï¸ I couldnâ€™t find any relevant answer for: **{query}**. Try rephrasing your question.\"\n",
        "\n",
        "    bullet_points = \"\\n\\n\".join(f\"- {doc.strip()}\" for doc in retrieved_docs)\n",
        "    return f\"\"\"### ðŸ§¾ Question:\\n**{query}**\n",
        "\n",
        "### ðŸ“˜ Answer Summary:\n",
        "{bullet_points}\n",
        "\"\"\"\n",
        "\n",
        "# Safe RAG call\n",
        "def ask_maternal_ai(query):\n",
        "    try:\n",
        "        qv = model.encode([query])[0]\n",
        "        results = collection.query(query_embeddings=[qv.tolist()], n_results=3)\n",
        "        return compose_answer(query, results)\n",
        "    except Exception as e:\n",
        "        return f\"âŒ Error during processing: {str(e)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8f162e3",
      "metadata": {
        "id": "f8f162e3"
      },
      "source": [
        "## Basic Evaluation (hit@3 on a tiny golden set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e71fecdf",
      "metadata": {
        "id": "e71fecdf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "golden = [\n",
        "    {\"q\":\"postpartum danger signs\",\"must\":[\"bleeding\",\"headache\",\"shortness of breath\"]},\n",
        "    {\"q\":\"prenatal care basics\",\"must\":[\"prenatal\",\"screenings\",\"visits\"]},\n",
        "]\n",
        "\n",
        "def retrieve_docs(q, k=3):\n",
        "    qv = model.encode([q])[0]\n",
        "    res = collection.query(query_embeddings=[qv.tolist()], n_results=k)\n",
        "    return res['documents'][0]\n",
        "\n",
        "hits = []\n",
        "for item in golden:\n",
        "    docs = retrieve_docs(item[\"q\"], k=3)\n",
        "    text = \" \".join(docs).lower()\n",
        "    ok = all(term in text for term in item[\"must\"])\n",
        "    hits.append(int(ok))\n",
        "\n",
        "print(\"âœ… hit@3:\", round(float(np.mean(hits)), 3), \"| per-item:\", hits)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What are the signs of postpartum depression?\"\n",
        "response = ask_maternal_ai(question)\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "kheLBWsBek3J"
      },
      "id": "kheLBWsBek3J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "lrmhKCFBU43N",
      "metadata": {
        "id": "lrmhKCFBU43N"
      },
      "source": [
        "# 7. Interactive Dashboard\n",
        "## 7.1 Interface: Gradio\n",
        "\n",
        "Implemented using Gradio with:\n",
        "\n",
        "Input box for questions\n",
        "\n",
        "Markdown output with formatted responses\n",
        "\n",
        "Clickable example queries\n",
        "\n",
        "## 7.2 Sample Output\n",
        "\n",
        "Question: What symptoms after birth require emergency care?\n",
        "Answer Summary:\n",
        "\n",
        "Seek immediate help for severe headache, chest pain, heavy bleeding, or shortness of breath."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C72IycXUQAIY",
      "metadata": {
        "id": "C72IycXUQAIY",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "sample_questions = [\n",
        "    \"What are the signs of postpartum depression?\",\n",
        "    \"How often should I get prenatal checkups?\",\n",
        "    \"What symptoms after birth require emergency care?\",\n",
        "    \"Where can I get low-cost maternal health support?\",\n",
        "    \"What is prenatal care and why is it important?\"\n",
        "]\n",
        "\n",
        "gr.Interface(\n",
        "    fn=ask_maternal_ai,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Ask about maternal health...\"),\n",
        "    outputs=gr.Markdown(label=\"AI Assistant Response\"),\n",
        "    title=\"ðŸ¤° MaternalCare AI Assistant\",\n",
        "    description=\"Answering common maternal health questions based on a curated knowledge base.\",\n",
        "    examples=sample_questions\n",
        ").launch(debug=True, share=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "i25OBASwVHLo",
      "metadata": {
        "id": "i25OBASwVHLo"
      },
      "source": [
        "# 8. Conclusion\n",
        "\n",
        "The development of MaternalCare_AI challenged us to apply data science and AI principles to a real-world domain with high social impact - maternal health. Our goal was to create an assistant that could offer plain-language, localized health guidance to underserved communities, particularly those in ZIP codes with high Social Vulnerability Index (SVI) scores. Throughout the project, we built a full pipeline that included curated knowledge base creation, semantic chunking, embedding generation, and retrieval-augmented QA through ChromaDB. The assistant was deployed via a Gradio interface to ensure usability for both non-technical users and health outreach workers. One of the key learning outcomes was how to balance simplicity with utility. We made conscious decisions to use free, open-source tools (e.g., Colab, MiniLM, CSV-based KB) to simulate real-world constraints. Our evaluation strategy went beyond just accuracy - we analyzed semantic correctness using a golden QA set and cosine similarity, giving us insights into how useful the answers truly were. Looking forward, we see clear potential for integrating user feedback, adding multilingual support, and expanding the assistant to other public health domains like immunization or pediatric care. The project not only helped us consolidate technical skills but deepened our understanding of equitable AI design.\n",
        "\n",
        "## Team Contributions\n",
        "Zahra: Led data preprocessing, developed Gradio UI, and designed the semantic evaluation framework.\n",
        "Virginia: Implemented embedding + retrieval pipeline, managed the SVI and maternal risk datasets, and oversaw model testing and validation.\n",
        "\n",
        "All team members participated in planning, documentation, and final editing. Collaboration was managed using Google Drive, Colab notebooks, and shared version checkpoints. Each member reviewed and approved all final deliverables."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "Centers for Disease Control and Prevention (CDC). (2023). Social Vulnerability Index (SVI) 2022 Database. U.S. Department of Health & Human Services.\n",
        "https://www.atsdr.cdc.gov/placeandhealth/svi/index.html\n",
        "\n",
        "Kaggle Contributor. (2020). Maternal Health Risk Data Set. [Dataset]. Kaggle.\n",
        "https://www.kaggle.com/datasets/andrewmvd/maternal-health-risk-data\n",
        "\n",
        "Reimers, N., & Gurevych, I. (2019). Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. arXiv preprint arXiv:1908.10084.\n",
        "https://arxiv.org/abs/1908.10084\n",
        "\n",
        "ChromaDB Team. (2023). Chroma: The AI-native open-source embedding database.\n",
        "https://www.trychroma.com/\n",
        "\n",
        "Gradio Team. (2023). Gradio: Build machine learning demos in Python.\n",
        "https://www.gradio.app/\n",
        "\n",
        "MiniLM: Wang, Y., Wei, F., Dong, L., Bao, H., Yang, N., & Zhou, M. (2020). MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers. arXiv preprint arXiv:2002.10957.\n",
        "https://arxiv.org/abs/2002.10957\n",
        "\n",
        "MD Data Miner Group. (2025). Golden QA Semantic Evaluation Dataset for MaternalCare_AI. Internal course project dataset. ITAI2377, Fall 2025."
      ],
      "metadata": {
        "id": "HfB17wRVspNm"
      },
      "id": "HfB17wRVspNm"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "663AFb6WswGp"
      },
      "id": "663AFb6WswGp",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}